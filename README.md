# SFTproject

In this project I aimed to evaluate the impact of the Supervised Fine-Tuning process on enhancing a model's ability to address mathematical questions.

## Datasets

I created two different datasets, each based on Wikipedia pages on mathematical topics but differing in size and using different large language models.
In both cases I provided the model with a Wikipedia article and asked it to generate only one question and answer pair based on the content.  
The first dataset contains 22,535 question-answer pairs, generated using the Llama3.1:8b-instruct-q8\_0 model.  
The second dataset includes 2,222 question-answer pairs, generated by Gemini 1.5 Pro.  
**This second dataset was much more accurate thanthe second one, containing only mathematical content and question-answer pairs more complete and detailed**

## Fine-Tuning

My starting pre-trained model was the meta-llama/Meta-Llama-3.1-8B model.  
First, I fine-tuned the model on both datasets for 1 epoch with a learning rate of $5e^{-5}$, using a QLoRA adapter.  
I used a DataCollatorForCompletionOnlyLM to train the model on the completions only, i.e., the part of the prompt after the string `### Answer:`.  
For both datasets, I have taken the 90% of the examples for my train dataset and the remaining 10% for the evaluation dataset. At the end of the training, I generated the responses of the new models on the questions of the respective evaluation dataset.

## Evaluation

To evaluate the accuracy of the generated answers I used the meta-llama/Llama-3-70b model in two separate calls. In the first call, I provided the correct answer along with the answer generated by the model without SFT. In the second call, I provided the correct answer along with the answer generated by the fine-tuned model.  
I repeated the same exact process with the meta-llama/Meta-Llama-3.1-8B-Instruct model.  
In both calls, I asked the model to determine whether the generated answer was correct compared to the first answer, even if the two responses were not exactly identical.

## Results

These are the results I obtained for the first dataset:  

| Model  | Accuracy |
| ---       | ---       |
| Base model (Llama-3.1-8b): |  51.7% |
| Base Instruct model (Llama-3.1-8b-Instruct):  | 51.1% |
| Our Fine-tuned model (Llama-3.1-8b-math):  | 58.9% |
| Our Fine-tuned Instruct model (Llama-3.1-8b-Instruct-math):  | 60.0% |

and these for the second one:  

| Model  | Accuracy |
| ---       | ---       |
| Base model (Llama-3.1-8b): |  59.0% |
| Base Instruct model (Llama-3.1-8b-Instruct):  | 57.7% |
| Our Fine-tuned model (Llama-3.1-8b-math):  | 59.5% |
| Our Fine-tuned Instruct model (Llama-3.1-8b-Instruct-math):  | 63.6% |

**We can see that, in general, the models achieve a better accuracy on the the Gemini dataset because of the quality of the data, even if the examples are much fewer.**  
However, I was not able to get an evident improvement between the base Llama-3.1-8b model with and without fine-tuning.
That is why I tried to train the model on the second dataset for more epochs, in order to see if its performances were actually improving.

## Training for more epochs

This time, I trained the model without adapters using the Together AI API, for 1, 3, 5, 7, 10, 15 and 20 epochs with a learning rate of $1e^{-5}$.  
I calculated the accuracy of my models across the various epochs of training by using the meta-llama/Llama-3-70b model again, making one call for each model, in the same way as before. These are the results:

| Epochs of training | Accuracy |
| ---       | ---       |
| 1 epoch | 58.1% |
| 3 epochs | 54.5% |
| 5 epochs | 55.4% |
| 7 epochs | 63.1% |
| 10 epochs | 64.5% |
| 15 epochs | 67.7% |
| 20 epochs | 65.4% |

I then wrote 10 almost basic math questions and personally compared the results of my models.  
I found that the models trained for multiple epochs are sometimes better than the base one in answering the given question but I achieved good results only after training the model for 15 epochs.
I noticed that the models trained for more 15 epochs are able to only provide answers, without generating additional questions or repeating content and they stop before reaching the maximum sequence length of the output.
Still these models sometimes generate multiple possible answers or they are not very accurate.

## Final Results

**The results demonstrate that fine-tuning the model, especially over multiple epochs, leads to noticeable improvements in accuracy and consistency in answering mathematical questions, as also observed in the InstructGPT and LIMA papers.** The model trained for 15 epochs performs better than the base model, avoiding redundant outputs and producing more reliable answers. However, even this occasionally generates imprecise responses, as seen in some examples where they fail to fully adhere to mathematical conventions or offer multiple possible answers.

Further fine-tuning and adjustments, such as refining the dataset, using a larger model, or refining the hyperparameters, could potentially improve the modelâ€™s precision. Additionally, incorporating this process into full RLHF could help guide the model towards better performance in specialized domains like mathematics.
